{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T14:37:35.905713Z",
     "start_time": "2019-03-01T14:37:34.874603Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import time\n",
    "import random\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Bot for Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Define the Scraping Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step define the function, which will scrape the website for flats and their attributes. The attributes that I am collecting are:\n",
    "\n",
    "- price\n",
    "- address\n",
    "- number of rooms\n",
    "- square metres\n",
    "- link of the flat\n",
    "- title, which includes a description of the flat\n",
    "- extra, which additional features of the flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T14:37:36.343052Z",
     "start_time": "2019-03-01T14:37:36.311823Z"
    }
   },
   "outputs": [],
   "source": [
    "def collect_data():\n",
    "    \n",
    "    # Prices\n",
    "    if house_containers[p].find('span', class_='resultlist-value') in (None, np.nan):\n",
    "        price = np.nan\n",
    "    else:\n",
    "        price = house_containers[p].find('span', class_='resultlist-value').text\n",
    "        price = re.sub('\\D', '', price)\n",
    "        price = float(price)\n",
    "        price = price / 100\n",
    "    prices.append(price)\n",
    "\n",
    "    # Addresses\n",
    "    if house_containers[p].find('div', class_='resultlist-address') in (None, np.nan):\n",
    "        address = np.nan\n",
    "    else:\n",
    "        address = house_containers[p].find('div', class_='resultlist-address').text\n",
    "        address = re.sub('\\s','',address)\n",
    "    addresses.append(address)\n",
    "\n",
    "    # Rooms\n",
    "    if house_containers[p].find_all('span', class_='resultlist-value')[2] in (None, np.nan):\n",
    "        room = np.nan\n",
    "    else:\n",
    "        room = house_containers[p].find_all('span', class_='resultlist-value')[2].text\n",
    "        room = re.sub(',','.',room)\n",
    "        room = re.search('(\\d+.\\d+)|(\\d+)', room).group()\n",
    "        room = float(room)\n",
    "    rooms.append(room)\n",
    "\n",
    "    # Square Metre\n",
    "    if house_containers[p].find_all('span', class_='resultlist-value')[1] in (None, np.nan):\n",
    "        sqm = np.nan\n",
    "    else:\n",
    "        sqm = house_containers[p].find_all('span', class_='resultlist-value')[1].text\n",
    "        sqm = re.sub(',','.',sqm)\n",
    "        sqm = re.search('(\\d+.\\d+)|(\\d+)', sqm).group()\n",
    "        sqm = float(sqm)\n",
    "    sqms.append(sqm)\n",
    "\n",
    "    # Links\n",
    "    if house_containers[p].find('a', class_='resultlist-title').get('href') in (None, np.nan):\n",
    "        href = np.nan\n",
    "    else:\n",
    "        href = house_containers[p].find('a', class_='resultlist-title').get('href')\n",
    "        href = 'https:'+href\n",
    "    hrefs.append(href)               \n",
    "\n",
    "    # Titles\n",
    "    if house_containers[p].find('a', class_='resultlist-title') in (None, np.nan):\n",
    "        title = np.nan\n",
    "    else:\n",
    "        title = house_containers[p].find('a', class_='resultlist-title').text\n",
    "    titles.append(title)\n",
    "\n",
    "    # Extra\n",
    "    if house_containers[p].find('ul', class_='resultlist-properties') in (None, np.nan):\n",
    "        extra = np.nan\n",
    "    else:\n",
    "        extra = house_containers[p].find('ul', class_='resultlist-properties').text\n",
    "        extra = re.sub('\\s','',extra)\n",
    "    extras.append(extra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Run the Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section I run the bot to scrape the website and insert the collected information into the appropriate lists. At thend I print how many properties I managed to scrape - 3266."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T14:46:15.736599Z",
     "start_time": "2019-03-01T14:37:36.343052Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You scraped 273 pages containing 3266 properties.\n"
     ]
    }
   ],
   "source": [
    "n_pages = 0\n",
    "header = ({'User-Agent': \n",
    "            'Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/41.0.2228.0 Safari/537.36'})\n",
    "\n",
    "prices = []\n",
    "addresses =[]\n",
    "rooms = []\n",
    "sqms = []\n",
    "hrefs = []\n",
    "titles = []\n",
    "extras = []\n",
    "\n",
    "\n",
    "\n",
    "for page in range(273):\n",
    "    n_pages += 1\n",
    "    if n_pages == 1:\n",
    "        url = 'https://www.immobilienscout24.de/wohnen/berlin,berlin/mietwohnungen.html'\n",
    "    else:\n",
    "        url = 'https://www.immobilienscout24.de/wohnen/berlin,berlin/mietwohnungen'+',seite-'+str(n_pages)+'.html'\n",
    "    page = requests.get(url, headers=header)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    house_containers = soup.find_all('div', class_='grid resultlist-container-big')\n",
    "    if house_containers != []:\n",
    "        for p in range(len(house_containers)):\n",
    "            \n",
    "            collect_data()\n",
    "                \n",
    "    else:\n",
    "        house_containers = soup.find_all('div', class_='grid resultlist-container')\n",
    "        for p in range(len(house_containers)):\n",
    "            \n",
    "            collect_data() \n",
    "        \n",
    "    time.sleep(random.randint(1,2))\n",
    "    \n",
    "print('You scraped {} pages containing {} properties.'.format(n_pages, len(addresses)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I reformat the data and save it all to one dataframe. I also create a new field caled 'square metre price'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T14:46:15.799087Z",
     "start_time": "2019-03-01T14:46:15.736599Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save to dataframe\n",
    "berlin_rent = pd.DataFrame({'address': addresses,\n",
    "                   'rooms': rooms,\n",
    "                   'area sqm': sqms,\n",
    "                   'price': prices,\n",
    "                   'links': hrefs,\n",
    "                   'title': titles,\n",
    "                   'extra': extras})\n",
    "\n",
    "# Extract postal code data \n",
    "postcode = [re.search('\\d+',berlin_rent['address'][r]).group() for r in range(len(berlin_rent))]\n",
    "\n",
    "# Add postal code data to the dataframe \n",
    "berlin_rent = pd.concat([berlin_rent, pd.Series(postcode).rename('postcode')], axis=1)\n",
    "\n",
    "# Create a new field: square metre price\n",
    "berlin_rent['price_sqm'] = berlin_rent['price'] / berlin_rent['area sqm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Save to csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I save everything to a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-01T14:46:16.299020Z",
     "start_time": "2019-03-01T14:46:16.252099Z"
    }
   },
   "outputs": [],
   "source": [
    "berlin_rent.to_csv('C:/Users/Alessandro/Desktop/Scraping/Berlin/berlin_rent.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
